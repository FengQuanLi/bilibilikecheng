{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个参数的神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络可以只有一个参数吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## $G=g*m$  \n",
    "物体受到的重力等于物体的质量乘以重力加速度,\n",
    "假设物体受到的力G和物体的质量m是测量来的.要确定常数g的值.即我们要确定的神经网络的参数.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用python实现一个参数的神经网络  \n",
    "为什么只有一个参数的网络可以直观的得出参数的值，却要用先构造损失函数，而后用梯度下降的的方法来求得参数的值？  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12265822483933908\n",
      "9.799999999999956\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 生成训练用的数据集 假设物体受到的力G和物体的质量m是测量得来\n",
    "单组数据 = {}\n",
    "数据集 = []\n",
    "for i in range(10000):\n",
    "    M = random.random()\n",
    "    g_real = M * 9.8\n",
    "    单组数据['m'] = M\n",
    "    单组数据['G_real'] = g_real\n",
    "    数据集.append(单组数据.copy())\n",
    "\n",
    "\n",
    "# 构造一个神经网络 G=mg\n",
    "class 质量求重力G():\n",
    "    def __init__(self):\n",
    "        # 初始化网络参数\n",
    "        self.g = random.random()\n",
    "    def forward(self,m):\n",
    "        #G = mg\n",
    "        G=self.g*m\n",
    "        return G\n",
    "    \n",
    "# 实例化一个神经网络\n",
    "G_预测=质量求重力G()\n",
    "print(G_预测.g)\n",
    "学习率=0.01\n",
    "for i in range(10000):\n",
    "    #取出一组数据\n",
    "    m=数据集[i]['m']\n",
    "    G_real=数据集[i]['G_real']\n",
    "    # 前向传播 G = mg\n",
    "    G_预测输出=G_预测.forward(m)\n",
    "    #损失函数\n",
    "    loss=(G_预测输出-G_real)**2\n",
    "    #求导 loss对g的导数\n",
    "    # G = mg\n",
    "    loss_g=2*(G_预测输出-G_real)*m\n",
    "    #反向传播\n",
    "    G_预测.g=G_预测.g-学习率*loss_g\n",
    "\n",
    "\n",
    "print(G_预测.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30027697868642134\n",
      "9.79998732512885\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 生成训练用的数据集 假设物体受到的力G和物体的质量m是测量得来\n",
    "单组数据 = {}\n",
    "数据集 = []\n",
    "for i in range(10000):\n",
    "    M = random.random()\n",
    "    g_real = M * 9.8\n",
    "    单组数据['m'] = M\n",
    "    单组数据['G_real'] = g_real\n",
    "    数据集.append(单组数据.copy())\n",
    "\n",
    "# G=mg\n",
    "# 构造一个神经网络\n",
    "class 质量求重力G():\n",
    "\n",
    "    def __init__(self):\n",
    "        # 初始化网络参数\n",
    "        self.g = random.random()\n",
    "    def forward(self,m):\n",
    "        #G = mg\n",
    "        G=self.g*m\n",
    "        return G\n",
    "# 实例化一个神经网络\n",
    "G_预测=质量求重力G()\n",
    "print(G_预测.g)\n",
    "学习率=0.01\n",
    "\n",
    "\n",
    "for i in range(2000):\n",
    "    # 前向传播\n",
    "    loss = 0\n",
    "    loss_g = 0\n",
    "    j = 0\n",
    "    for 单组数据 in 数据集:\n",
    "        m = 单组数据['m']\n",
    "        G_real = 单组数据['G_real']\n",
    "        G_预测输出 = G_预测.forward(m)\n",
    "        loss = loss + (G_预测输出 - G_real) ** 2\n",
    "        loss_g = loss_g + 2 * (G_预测输出 - G_real) * m\n",
    "        j = j + 1\n",
    "    loss = loss / j\n",
    "    loss_g = loss_g / j\n",
    "    # 反向传播\n",
    "    G_预测.g = G_预测.g-  学习率 * loss_g\n",
    "\n",
    "print(G_预测.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、构造一个单个参数的神经网络G=gm。\n",
    "### 2、目的是求得适合的参数g使得gm等于测量得来的重力G测。\n",
    "### 3、条件是一系列测量得来的m和G测的数据\n",
    "### 4、构造损失函数loss=(gm-G测)**2\n",
    "### 5、目的等价为求以g为变量的损失函数loss有最小值的g值。\n",
    "### 6、先求loss对g的导数在模型目前取值g0情况下的具体值。\n",
    "### 7、如果导数在g0点是负值，那么函数沿着g轴方向是下坡,g0增加一个微小的数值就能使得loss下降。如果导数在g0点是正值，那么函数沿着g轴负方向是下坡,g0减小一个微小的数值就能使得loss下降。\n",
    "### g0=g0-学习率 *loss_g0导 就可实现一步迭代。\n",
    "### 8、完成多次迭代就可逼近损失函数loss的最小值，这个多次迭代的g值就是我们要求得的g。\n",
    "### 9、意像性的解释。不严谨处，不再赘述。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是最小二乘法？\n",
    "[传送门1](https://baike.baidu.com/item/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/2522346?fr=aladdin   ) \n",
    "\n",
    "\n",
    "什么是均方差mseloss?\n",
    "[传送门1 ](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)   \n",
    "\n",
    "\n",
    "什么是导数？\n",
    "[传送门1](https://baike.baidu.com/item/%E5%AF%BC%E6%95%B0/579188?fr=aladdin )  \n",
    "\n",
    "\n",
    "什么是链式法则？\n",
    " [传送门1](https://baike.baidu.com/item/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99/3314017?fr=aladdin )\n",
    "[传送门2](https://www.bilibili.com/video/BV1864y1T7Ks?p=52   )    \n",
    "\n",
    "\n",
    "什么是梯度？\n",
    "[传送门1](https://www.bilibili.com/video/BV1864y1T7Ks?p=164)\n",
    "[传送门2](https://baike.baidu.com/item/%E6%A2%AF%E5%BA%A6/13014729?fr=aladdin)    \n",
    "\n",
    "\n",
    "\n",
    "什么是梯度下降法？  \n",
    "gradient descent  \n",
    "什么是随机梯度下降法？\n",
    "[传送门](https://zhidao.baidu.com/question/754577176406185724.html)  \n",
    "\n",
    "[B站上的python课程](https://www.bilibili.com/video/BV1c4411e77t)   \n",
    "[环境搭建](https://www.bilibili.com/video/BV1h64y1s7Ae) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个函数，如果函数某处有导数,那么此处的导数就是过此处的切线的斜率\n",
    "[传送门](https://baike.baidu.com/item/%E5%88%87%E7%BA%BF/674562?fr=aladdin )  \n",
    "两个函数和的导数等于两个函数导数的和。  [传送门](https://zhidao.baidu.com/question/688537209916157532.html)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
